{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efece79a",
   "metadata": {},
   "source": [
    "# Résumé automatique (FR) — Modèle perso from scratch\n",
    "\n",
    "Objectif :\n",
    "- Construire une IA de synthèse (texte → résumé) en français\n",
    "- Tokenisation standard via SentencePiece (BPE/Unigram)\n",
    "- Modèle perso : Transformer Encoder–Decoder (from scratch)\n",
    "- Entraînement supervisé sur (text, summary)\n",
    "- Évaluation : ROUGE + exemples qualitatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "083e4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3656690",
   "metadata": {},
   "source": [
    "## Device, seeds, hyperparams (Markdown + Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3f4509",
   "metadata": {},
   "source": [
    "On fixe un environnement reproductible (seed), on choisit le device (GPU si dispo),\n",
    "et on définit des hyperparamètres \"MVP\" qui tournent sans exploser la VRAM.\n",
    "Le modèle from scratch doit rester petit (d_model=256) sinon il apprend lentement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "264a7952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df7aadd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins\n",
    "TRAIN_CSV = \"./fr_train.csv\"\n",
    "TEST_CSV = \"./fr_test.csv\"\n",
    "\n",
    "# Tokenisation / séquences\n",
    "MAX_IN_TOKENS = 512\n",
    "MAX_OUT_TOKENS = 160\n",
    "VOCAB_SIZE = 16000\n",
    "\n",
    "# Modèle Transformer mini\n",
    "D_MODEL = 256\n",
    "N_HEADS = 4\n",
    "ENC_LAYERS = 4\n",
    "DEC_LAYERS = 4\n",
    "D_FF = 1024\n",
    "DROPOUT = 0.15  # ↑ régularise mieux\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 16  # ↑ meilleure stabilité\n",
    "LR = 5e-4  # ↓ plus stable (3e-4 trop élevé)\n",
    "EPOCHS = 12  # ↓ suffisant (50 était trop long)\n",
    "LABEL_SMOOTH = 0.1\n",
    "GRAD_CLIP = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06df002",
   "metadata": {},
   "source": [
    "## Construire le corpus pour SentencePiece (Markdown + Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e99825",
   "metadata": {},
   "source": [
    "SentencePiece nécessite un corpus texte brut pour apprendre un vocabulaire subword.\n",
    "Nous construisons donc un fichier intermédiaire `spm_corpus.txt` à partir du jeu\n",
    "d'entraînement, en y plaçant les textes d'entrée et les résumés de référence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acbd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# chemins (à adapter si besoin)\n",
    "TRAIN_CSV = \"./fr_train.csv\"   # ton vrai fichier\n",
    "corpus_path = \"spm_corpus.txt\"    # sera CRÉÉ ici\n",
    "\n",
    "# charge le train\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# LIMITE LE DATASET À 1000 ROWS POUR ACCÉLÉRER LE TRAITEMENT\n",
    "train_df = train_df.head(5000)\n",
    "\n",
    "# construit le corpus texte pour SentencePiece\n",
    "with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in train_df.iterrows():\n",
    "        # entrée : titre + texte\n",
    "        if \"title\" in train_df.columns:\n",
    "            src = f\"titre: {row['title']} texte: {row['text']}\"\n",
    "        else:\n",
    "            src = row[\"text\"]\n",
    "\n",
    "        # cible : résumé\n",
    "        tgt = row[\"summary\"]\n",
    "\n",
    "        # SentencePiece veut une phrase par ligne\n",
    "        f.write(str(src).replace(\"\\n\", \" \") + \"\\n\")\n",
    "        f.write(str(tgt).replace(\"\\n\", \" \") + \"\\n\")\n",
    "\n",
    "print(\"✅ spm_corpus.txt créé\")\n",
    "print(\"Taille (lignes):\", sum(1 for _ in open(corpus_path, encoding=\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03581f3a",
   "metadata": {},
   "source": [
    "On entraîne un tokenizer **standard** (SentencePiece) sur `spm_corpus.txt`.  \n",
    "Pourquoi : un modèle from scratch a besoin d'une tokenisation robuste (subwords) pour gérer vocabulaire, accents, mots rares.\n",
    "\n",
    "Sorties :\n",
    "- `spm_fr.model` : le tokenizer à charger dans tout le projet\n",
    "- `spm_fr.vocab` : vocabulaire lisible (debug + rapport)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c8eb450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\aurel\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (0.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a34422b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer déjà présent: spm_fr.model\n"
     ]
    }
   ],
   "source": [
    "# Si besoin: !pip -q install sentencepiece\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "CORPUS_PATH = \"spm_corpus.txt\"\n",
    "MODEL_PREFIX = \"spm_fr\"  # => spm_fr.model + spm_fr.vocab\n",
    "\n",
    "VOCAB_SIZE = 16000\n",
    "\n",
    "if not os.path.exists(CORPUS_PATH):\n",
    "    raise FileNotFoundError(f\"Je ne trouve pas {CORPUS_PATH}\")\n",
    "\n",
    "if not os.path.exists(MODEL_PREFIX + \".model\"):\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=CORPUS_PATH,\n",
    "        model_prefix=MODEL_PREFIX,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        model_type=\"unigram\",      # \"bpe\" marche aussi\n",
    "        character_coverage=1.0,    # français\n",
    "        pad_id=0, unk_id=1, bos_id=2, eos_id=3\n",
    "    )\n",
    "    print(\"✅ Tokenizer entraîné:\", MODEL_PREFIX + \".model\")\n",
    "else:\n",
    "    print(\"Tokenizer déjà présent:\", MODEL_PREFIX + \".model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376644a8",
   "metadata": {},
   "source": [
    "## Charger le tokenizer et valider encode/decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d28328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 16000\n",
      "ids: [38, 7029, 22, 2187, 4, 87, 9, 5330, 139, 2306, 4567, 233, 12, 11871, 967, 8, 7]\n",
      "decode: Le PSG a gagné, mais la météo était bizarre à Colombes.\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(MODEL_PREFIX + \".model\")\n",
    "\n",
    "print(\"Vocab size:\", sp.get_piece_size())\n",
    "\n",
    "txt = \"Le PSG a gagné, mais la météo était bizarre à Colombes.\"\n",
    "ids = sp.encode_as_ids(txt)\n",
    "print(\"ids:\", ids[:30])\n",
    "print(\"decode:\", sp.decode_ids(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a3b3e",
   "metadata": {},
   "source": [
    "## Charger les CSV train/test + vérifier les colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba12385",
   "metadata": {},
   "source": [
    "On charge les datasets. On attend au minimum `text` et `summary`.\n",
    "Optionnel mais utile : `title` (on l'injecte dans l'entrée pour aider from scratch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7f513eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (392902, 6)\n",
      "test : (15828, 6)\n",
      "cols : ['text', 'summary', 'topic', 'url', 'title', 'date']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jean-Jacques Schuhl, Gilles Leroy, Christian G...</td>\n",
       "      <td>Jean-Jacques Schuhl, Gilles Leroy, Christian G...</td>\n",
       "      <td>livres</td>\n",
       "      <td>https://www.lemonde.fr/livres/article/2010/01/...</td>\n",
       "      <td>La rentrée littéraire promet un programme de b...</td>\n",
       "      <td>01/01/2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Une semaine après l'attaque terroriste manquée...</td>\n",
       "      <td>Cette demande intervient une semaine après l'a...</td>\n",
       "      <td>proche-orient</td>\n",
       "      <td>https://www.lemonde.fr/proche-orient/article/2...</td>\n",
       "      <td>Gordon Brown appelle à une réunion internation...</td>\n",
       "      <td>01/01/2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Jean-Jacques Schuhl, Gilles Leroy, Christian G...   \n",
       "1  Une semaine après l'attaque terroriste manquée...   \n",
       "\n",
       "                                             summary          topic  \\\n",
       "0  Jean-Jacques Schuhl, Gilles Leroy, Christian G...         livres   \n",
       "1  Cette demande intervient une semaine après l'a...  proche-orient   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.lemonde.fr/livres/article/2010/01/...   \n",
       "1  https://www.lemonde.fr/proche-orient/article/2...   \n",
       "\n",
       "                                               title        date  \n",
       "0  La rentrée littéraire promet un programme de b...  01/01/2010  \n",
       "1  Gordon Brown appelle à une réunion internation...  01/01/2010  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print(\"train:\", train_df.shape)\n",
    "print(\"test :\", test_df.shape)\n",
    "print(\"cols :\", list(train_df.columns))\n",
    "\n",
    "required = {\"text\", \"summary\"}\n",
    "missing = required - set(train_df.columns)\n",
    "assert not missing, f\"Colonnes manquantes: {missing}\"\n",
    "\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db920a6e",
   "metadata": {},
   "source": [
    "## Vérifier les longueurs en tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e81c67",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "On mesure p50/p90/p95 des longueurs en tokens sur un échantillon.\n",
    "Pourquoi : confirmer que MAX_IN_TOKENS/MAX_OUT_TOKENS ne tronquent pas trop et ne gaspillent pas la VRAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f698f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN  tokens p50/p90/p95: [ 699 1471 1825]\n",
      "OUT tokens p50/p90/p95: [36 53 60]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_src(row):\n",
    "    if \"title\" in row.index and pd.notna(row[\"title\"]):\n",
    "        return f\"titre: {row['title']} texte: {row['text']}\"\n",
    "    return f\"texte: {row['text']}\"\n",
    "\n",
    "def token_len(s: str) -> int:\n",
    "    return len(sp.encode_as_ids(str(s)))\n",
    "\n",
    "sample = train_df.sample(min(5000, len(train_df)), random_state=SEED)\n",
    "\n",
    "in_lens = sample.apply(lambda r: token_len(make_src(r)), axis=1).to_numpy()\n",
    "out_lens = sample[\"summary\"].apply(token_len).to_numpy()\n",
    "\n",
    "print(\"IN  tokens p50/p90/p95:\", np.percentile(in_lens, [50,90,95]).astype(int))\n",
    "print(\"OUT tokens p50/p90/p95:\", np.percentile(out_lens,[50,90,95]).astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db38435",
   "metadata": {},
   "source": [
    "## Dataset + batching (padding, masks, teacher forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca9782d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "On convertit chaque exemple en séquences d'IDs et on pad pour faire des batches GPU.\n",
    "On prépare aussi `dec_in` (summary décalé) pour teacher forcing :\n",
    "- entrée du décodeur : [BOS] + y[:-1]\n",
    "- labels : y\n",
    "Pourquoi : le décodeur apprend à prédire le prochain token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1199486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "PAD, UNK, BOS, EOS = 0, 1, 2, 3\n",
    "\n",
    "def encode_src(row):\n",
    "    src = make_src(row)\n",
    "    ids = [BOS] + sp.encode_as_ids(src)[:MAX_IN_TOKENS-2] + [EOS]\n",
    "    return ids\n",
    "\n",
    "def encode_tgt(row):\n",
    "    tgt = str(row[\"summary\"])\n",
    "    ids = [BOS] + sp.encode_as_ids(tgt)[:MAX_OUT_TOKENS-2] + [EOS]\n",
    "    return ids\n",
    "\n",
    "def pad_to(ids, max_len, pad_id=PAD):\n",
    "    return ids + [pad_id]*(max_len-len(ids)) if len(ids) < max_len else ids[:max_len]\n",
    "\n",
    "class SumDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        return encode_src(row), encode_tgt(row)\n",
    "\n",
    "@dataclass\n",
    "class Batch:\n",
    "    src_ids: torch.Tensor\n",
    "    dec_in: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    xs = [pad_to(x, MAX_IN_TOKENS) for x in xs]\n",
    "    ys = [pad_to(y, MAX_OUT_TOKENS) for y in ys]\n",
    "\n",
    "    src_ids = torch.tensor(xs, dtype=torch.long)\n",
    "    labels  = torch.tensor(ys, dtype=torch.long)\n",
    "\n",
    "    dec_in = labels.clone()\n",
    "    dec_in[:, 1:] = labels[:, :-1]\n",
    "    dec_in[:, 0] = BOS\n",
    "\n",
    "    return Batch(src_ids=src_ids, dec_in=dec_in, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17740912",
   "metadata": {},
   "source": [
    "## Split train/val + DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3b054",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "On garde une validation pour suivre l'apprentissage et sauvegarder le meilleur modèle.\n",
    "Pourquoi : from scratch peut sur-apprendre ou diverger, donc on checkpoint sur val_loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb4f17f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (392902, 6)\n",
      "val  : (16059, 6)\n",
      "test : (15828, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24557, 1004, 990)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAL_CSV = \"./fr_validation.csv\"   # <-- adapte le nom (ex: fr_valid.csv, validation.csv...)\n",
    "\n",
    "val_df = pd.read_csv(VAL_CSV)\n",
    "\n",
    "print(\"train:\", train_df.shape)\n",
    "print(\"val  :\", val_df.shape)\n",
    "print(\"test :\", test_df.shape)\n",
    "\n",
    "# sanity check colonnes\n",
    "required = {\"text\", \"summary\"}\n",
    "for name, df in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
    "    missing = required - set(df.columns)\n",
    "    assert not missing, f\"Colonnes manquantes dans {name}: {missing}\"\n",
    "\n",
    "train_loader = DataLoader(SumDataset(train_df), batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(SumDataset(val_df),   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(SumDataset(test_df),  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d063d2d",
   "metadata": {},
   "source": [
    "## Masque causal (le décodeur ne voit pas le futur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ab2d9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Le décodeur génère token par token. Il ne doit pas accéder aux tokens futurs.\n",
    "On crée donc un masque triangulaire (causal) pour le self-attention du décodeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b36ff9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(T: int, device):\n",
    "    # True = autorisé ; PyTorch attend souvent True=mask => on inversera dans le forward\n",
    "    return torch.tril(torch.ones((T, T), dtype=torch.bool, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b69a4",
   "metadata": {},
   "source": [
    "## Modèle perso : Transformer Encoder–Decoder (mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33577be",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Architecture moderne du résumé abstractive :\n",
    "- embeddings appris + positional encoding\n",
    "- encoder : self-attention\n",
    "- decoder : masked self-attention + cross-attention\n",
    "Pourquoi : c'est la base des modèles de synthèse, mais ici entraînée from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47b10a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=4096):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerSummarizer(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, D_MODEL, padding_idx=PAD)\n",
    "        self.pos = PositionalEncoding(D_MODEL, DROPOUT)\n",
    "\n",
    "        self.tf = nn.Transformer(\n",
    "            d_model=D_MODEL,\n",
    "            nhead=N_HEADS,\n",
    "            num_encoder_layers=ENC_LAYERS,\n",
    "            num_decoder_layers=DEC_LAYERS,\n",
    "            dim_feedforward=D_FF,\n",
    "            dropout=DROPOUT,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.lm_head = nn.Linear(D_MODEL, vocab_size)\n",
    "\n",
    "    def forward(self, src_ids, dec_in):\n",
    "        B, S = src_ids.shape\n",
    "        _, T = dec_in.shape\n",
    "\n",
    "        src_kpm = (src_ids == PAD)  # True where pad\n",
    "        tgt_kpm = (dec_in  == PAD)\n",
    "\n",
    "        src = self.pos(self.emb(src_ids))\n",
    "        tgt = self.pos(self.emb(dec_in))\n",
    "\n",
    "        cm = causal_mask(T, src_ids.device)  # True allowed\n",
    "        tgt_mask = ~cm                       # PyTorch: True = masked\n",
    "\n",
    "        h = self.tf(\n",
    "            src, tgt,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_kpm,\n",
    "            tgt_key_padding_mask=tgt_kpm,\n",
    "            memory_key_padding_mask=src_kpm\n",
    "        )\n",
    "        return self.lm_head(h)  # (B,T,V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9cd17a",
   "metadata": {},
   "source": [
    "## Loss avec label smoothing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f2a349",
   "metadata": {},
   "source": [
    "Label smoothing stabilise l'entraînement from scratch (moins d'overconfidence).\n",
    "On ignore PAD pour ne pas apprendre sur du vide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a792467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoothed_ce(logits, target, eps=0.1, ignore_index=PAD):\n",
    "    B, T, V = logits.shape\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    nll = F.nll_loss(\n",
    "        log_probs.view(B*T, V),\n",
    "        target.view(B*T),\n",
    "        reduction=\"none\",\n",
    "        ignore_index=ignore_index\n",
    "    ).view(B, T)\n",
    "\n",
    "    smooth = -log_probs.mean(dim=-1)\n",
    "\n",
    "    pad_mask = (target == ignore_index)\n",
    "    nll = nll.masked_fill(pad_mask, 0.0)\n",
    "    smooth = smooth.masked_fill(pad_mask, 0.0)\n",
    "\n",
    "    denom = (~pad_mask).sum().clamp(min=1)\n",
    "    return ((1-eps)*nll + eps*smooth).sum() / denom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c787b",
   "metadata": {},
   "source": [
    "## Initialiser modèle + optimiseur\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f220b362",
   "metadata": {},
   "source": [
    "On instancie le modèle, on le met sur GPU si dispo, et on prépare AdamW.\n",
    "On affiche aussi le nombre de paramètres (utile dans le rapport)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12620d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: 15581824 (15.58 M)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aurel\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = TransformerSummarizer(vocab_size=sp.get_piece_size()).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"params:\", n_params, f\"({n_params/1e6:.2f} M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1de26d",
   "metadata": {},
   "source": [
    "## Entraînement + checkpoint du meilleur modèle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea22397",
   "metadata": {},
   "source": [
    "Boucle seq2seq :\n",
    "- forward (teacher forcing via dec_in)\n",
    "- loss\n",
    "- backward\n",
    "- clip gradients\n",
    "- val_loss\n",
    "Pourquoi : garder le meilleur modèle pour l'inférence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "edf91f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Réglages: combien de batches max par \"epoch\"\n",
    "MAX_TRAIN_BATCHES = 1000  # ↑ plus de données par epoch\n",
    "MAX_VAL_BATCHES   = 300    # validation plus courte\n",
    "\n",
    "BEST_PATH = \"./best_transformer_summarizer.pt\"\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "\n",
    "def run_epoch_limited(loader, train=True, log_every=50, max_batches=None):\n",
    "    model.train(train)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    t_start = time.time()\n",
    "    last_log = t_start\n",
    "\n",
    "    # nombre réel de batches qu'on va faire\n",
    "    target_batches = len(loader) if max_batches is None else min(len(loader), max_batches)\n",
    "\n",
    "    for i, batch in enumerate(loader, start=1):\n",
    "        if max_batches is not None and i > max_batches:\n",
    "            break\n",
    "\n",
    "        src = batch.src_ids.to(device)\n",
    "        dec = batch.dec_in.to(device)\n",
    "        y   = batch.labels.to(device)\n",
    "\n",
    "        logits = model(src, dec)\n",
    "        loss = label_smoothed_ce(\n",
    "            logits, y,\n",
    "            eps=LABEL_SMOOTH,\n",
    "            ignore_index=PAD\n",
    "        )\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            optimizer.step()\n",
    "\n",
    "        n_tok = (y != PAD).sum().item()\n",
    "        total_loss += loss.item() * n_tok\n",
    "        total_tokens += n_tok\n",
    "\n",
    "        # LOGGING\n",
    "        if i == 1:\n",
    "            print(\"→ first batch OK\")\n",
    "\n",
    "        if (i % log_every == 0) or (i == target_batches):\n",
    "            now = time.time()\n",
    "            dt = now - last_log\n",
    "            elapsed = now - t_start\n",
    "\n",
    "            avg_loss = total_loss / max(1, total_tokens)\n",
    "            # vitesse basée sur le nombre de batches depuis le dernier log\n",
    "            steps_since_last = log_every if i % log_every == 0 else (i % log_every)\n",
    "            speed = steps_since_last / max(dt, 1e-6)\n",
    "\n",
    "            pct = 100 * i / target_batches\n",
    "            eta = elapsed * (target_batches / i - 1)\n",
    "\n",
    "            print(\n",
    "                f\"[{'TRAIN' if train else 'VAL'}] \"\n",
    "                f\"batch {i:5d}/{target_batches} | \"\n",
    "                f\"{pct:5.1f}% | \"\n",
    "                f\"loss {avg_loss:.4f} | \"\n",
    "                f\"{speed:.2f} batches/s | \"\n",
    "                f\"ETA {eta/60:.1f} min\"\n",
    "            )\n",
    "\n",
    "            last_log = now\n",
    "\n",
    "    return total_loss / max(1, total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6b747",
   "metadata": {},
   "source": [
    "## Lancement de l'entraînement avec logging détaillé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c97173",
   "metadata": {},
   "source": [
    "On lance l'entraînement epoch par epoch en utilisant `run_epoch`.\n",
    "Le logging affiche :\n",
    "- confirmation du premier batch\n",
    "- progression (%)\n",
    "- loss moyenne\n",
    "- vitesse (batches/s)\n",
    "- ETA estimée\n",
    "\n",
    "Objectif : vérifier que l'entraînement avance et diagnostiquer les performances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f91951d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Début entraînement (batch limitées)\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2070 Super with Max-Q Design\n",
      "\n",
      "==========================================================================================\n",
      "Epoch 1/12 (limité à 1000 batches train, 300 val)\n",
      "→ first batch OK\n",
      "[TRAIN] batch    50/1000 |   5.0% | loss 7.7284 | 3.00 batches/s | ETA 5.3 min\n",
      "[TRAIN] batch   100/1000 |  10.0% | loss 7.3124 | 3.39 batches/s | ETA 4.7 min\n",
      "[TRAIN] batch   150/1000 |  15.0% | loss 7.1098 | 4.18 batches/s | ETA 4.1 min\n",
      "[TRAIN] batch   200/1000 |  20.0% | loss 6.9633 | 4.16 batches/s | ETA 3.7 min\n",
      "[TRAIN] batch   250/1000 |  25.0% | loss 6.8582 | 4.40 batches/s | ETA 3.3 min\n",
      "[TRAIN] batch   300/1000 |  30.0% | loss 6.7763 | 4.35 batches/s | ETA 3.0 min\n",
      "[TRAIN] batch   350/1000 |  35.0% | loss 6.7111 | 4.37 batches/s | ETA 2.8 min\n",
      "[TRAIN] batch   400/1000 |  40.0% | loss 6.6559 | 4.28 batches/s | ETA 2.5 min\n",
      "[TRAIN] batch   450/1000 |  45.0% | loss 6.6137 | 4.12 batches/s | ETA 2.3 min\n",
      "[TRAIN] batch   500/1000 |  50.0% | loss 6.5683 | 4.11 batches/s | ETA 2.1 min\n",
      "[TRAIN] batch   550/1000 |  55.0% | loss 6.5283 | 3.92 batches/s | ETA 1.9 min\n",
      "[TRAIN] batch   600/1000 |  60.0% | loss 6.4940 | 3.47 batches/s | ETA 1.7 min\n",
      "[TRAIN] batch   650/1000 |  65.0% | loss 6.4629 | 4.08 batches/s | ETA 1.5 min\n",
      "[TRAIN] batch   700/1000 |  70.0% | loss 6.4373 | 3.74 batches/s | ETA 1.3 min\n",
      "[TRAIN] batch   750/1000 |  75.0% | loss 6.4093 | 3.46 batches/s | ETA 1.1 min\n",
      "[TRAIN] batch   800/1000 |  80.0% | loss 6.3847 | 3.86 batches/s | ETA 0.9 min\n",
      "[TRAIN] batch   850/1000 |  85.0% | loss 6.3618 | 3.62 batches/s | ETA 0.6 min\n",
      "[TRAIN] batch   900/1000 |  90.0% | loss 6.3388 | 3.53 batches/s | ETA 0.4 min\n",
      "[TRAIN] batch   950/1000 |  95.0% | loss 6.3179 | 3.77 batches/s | ETA 0.2 min\n",
      "[TRAIN] batch  1000/1000 | 100.0% | loss 6.2983 | 4.08 batches/s | ETA 0.0 min\n",
      "→ first batch OK\n",
      "[VAL] batch    50/300 |  16.7% | loss 5.7496 | 14.24 batches/s | ETA 0.3 min\n",
      "[VAL] batch   100/300 |  33.3% | loss 5.7426 | 14.32 batches/s | ETA 0.2 min\n",
      "[VAL] batch   150/300 |  50.0% | loss 5.7462 | 14.35 batches/s | ETA 0.2 min\n",
      "[VAL] batch   200/300 |  66.7% | loss 5.7423 | 14.16 batches/s | ETA 0.1 min\n",
      "[VAL] batch   250/300 |  83.3% | loss 5.7475 | 14.27 batches/s | ETA 0.1 min\n",
      "[VAL] batch   300/300 | 100.0% | loss 5.7530 | 13.81 batches/s | ETA 0.0 min\n",
      "\n",
      " Résumé Epoch 1 | train_loss=6.2983 | val_loss=5.7530\n",
      " Nouveau meilleur modèle sauvegardé: ./best_transformer_summarizer.pt\n",
      "\n",
      "==========================================================================================\n",
      "Epoch 2/12 (limité à 1000 batches train, 300 val)\n",
      "→ first batch OK\n",
      "[TRAIN] batch    50/1000 |   5.0% | loss 5.8798 | 3.60 batches/s | ETA 4.4 min\n",
      "[TRAIN] batch   100/1000 |  10.0% | loss 5.8787 | 3.78 batches/s | ETA 4.1 min\n",
      "[TRAIN] batch   150/1000 |  15.0% | loss 5.8771 | 4.23 batches/s | ETA 3.7 min\n",
      "[TRAIN] batch   200/1000 |  20.0% | loss 5.8706 | 4.23 batches/s | ETA 3.4 min\n",
      "[TRAIN] batch   250/1000 |  25.0% | loss 5.8530 | 4.20 batches/s | ETA 3.1 min\n",
      "[TRAIN] batch   300/1000 |  30.0% | loss 5.8464 | 4.10 batches/s | ETA 2.9 min\n",
      "[TRAIN] batch   350/1000 |  35.0% | loss 5.8436 | 3.52 batches/s | ETA 2.8 min\n",
      "[TRAIN] batch   400/1000 |  40.0% | loss 5.8361 | 4.13 batches/s | ETA 2.5 min\n",
      "[TRAIN] batch   450/1000 |  45.0% | loss 5.8302 | 4.46 batches/s | ETA 2.3 min\n",
      "[TRAIN] batch   500/1000 |  50.0% | loss 5.8245 | 4.43 batches/s | ETA 2.1 min\n",
      "[TRAIN] batch   550/1000 |  55.0% | loss 5.8184 | 3.76 batches/s | ETA 1.9 min\n",
      "[TRAIN] batch   600/1000 |  60.0% | loss 5.8123 | 4.04 batches/s | ETA 1.7 min\n",
      "[TRAIN] batch   650/1000 |  65.0% | loss 5.8066 | 4.06 batches/s | ETA 1.5 min\n",
      "[TRAIN] batch   700/1000 |  70.0% | loss 5.7995 | 4.34 batches/s | ETA 1.2 min\n",
      "[TRAIN] batch   750/1000 |  75.0% | loss 5.7919 | 3.19 batches/s | ETA 1.0 min\n",
      "[TRAIN] batch   800/1000 |  80.0% | loss 5.7868 | 4.07 batches/s | ETA 0.8 min\n",
      "[TRAIN] batch   850/1000 |  85.0% | loss 5.7802 | 3.94 batches/s | ETA 0.6 min\n",
      "[TRAIN] batch   900/1000 |  90.0% | loss 5.7737 | 3.85 batches/s | ETA 0.4 min\n",
      "[TRAIN] batch   950/1000 |  95.0% | loss 5.7675 | 4.13 batches/s | ETA 0.2 min\n",
      "[TRAIN] batch  1000/1000 | 100.0% | loss 5.7624 | 3.80 batches/s | ETA 0.0 min\n",
      "→ first batch OK\n",
      "[VAL] batch    50/300 |  16.7% | loss 5.4946 | 9.90 batches/s | ETA 0.4 min\n",
      "[VAL] batch   100/300 |  33.3% | loss 5.4891 | 10.26 batches/s | ETA 0.3 min\n",
      "[VAL] batch   150/300 |  50.0% | loss 5.4972 | 13.64 batches/s | ETA 0.2 min\n",
      "[VAL] batch   200/300 |  66.7% | loss 5.4922 | 13.85 batches/s | ETA 0.1 min\n",
      "[VAL] batch   250/300 |  83.3% | loss 5.4969 | 13.89 batches/s | ETA 0.1 min\n",
      "[VAL] batch   300/300 | 100.0% | loss 5.5024 | 12.97 batches/s | ETA 0.0 min\n",
      "\n",
      " Résumé Epoch 2 | train_loss=5.7624 | val_loss=5.5024\n",
      " Nouveau meilleur modèle sauvegardé: ./best_transformer_summarizer.pt\n",
      "\n",
      "==========================================================================================\n",
      "Epoch 3/12 (limité à 1000 batches train, 300 val)\n",
      "→ first batch OK\n",
      "[TRAIN] batch    50/1000 |   5.0% | loss 5.6372 | 4.11 batches/s | ETA 3.9 min\n",
      "[TRAIN] batch   100/1000 |  10.0% | loss 5.6135 | 3.81 batches/s | ETA 3.8 min\n",
      "[TRAIN] batch   150/1000 |  15.0% | loss 5.6051 | 3.28 batches/s | ETA 3.8 min\n",
      "[TRAIN] batch   200/1000 |  20.0% | loss 5.6104 | 3.67 batches/s | ETA 3.6 min\n",
      "[TRAIN] batch   250/1000 |  25.0% | loss 5.6073 | 4.00 batches/s | ETA 3.3 min\n",
      "[TRAIN] batch   300/1000 |  30.0% | loss 5.6102 | 4.02 batches/s | ETA 3.1 min\n",
      "[TRAIN] batch   350/1000 |  35.0% | loss 5.6080 | 3.71 batches/s | ETA 2.9 min\n",
      "[TRAIN] batch   400/1000 |  40.0% | loss 5.6071 | 2.92 batches/s | ETA 2.7 min\n",
      "[TRAIN] batch   450/1000 |  45.0% | loss 5.6078 | 3.26 batches/s | ETA 2.5 min\n",
      "[TRAIN] batch   500/1000 |  50.0% | loss 5.6035 | 3.24 batches/s | ETA 2.3 min\n",
      "[TRAIN] batch   550/1000 |  55.0% | loss 5.6004 | 3.47 batches/s | ETA 2.1 min\n",
      "[TRAIN] batch   600/1000 |  60.0% | loss 5.5958 | 3.84 batches/s | ETA 1.9 min\n",
      "[TRAIN] batch   650/1000 |  65.0% | loss 5.5900 | 3.38 batches/s | ETA 1.6 min\n",
      "[TRAIN] batch   700/1000 |  70.0% | loss 5.5863 | 3.59 batches/s | ETA 1.4 min\n",
      "[TRAIN] batch   750/1000 |  75.0% | loss 5.5807 | 3.99 batches/s | ETA 1.2 min\n",
      "[TRAIN] batch   800/1000 |  80.0% | loss 5.5796 | 4.35 batches/s | ETA 0.9 min\n",
      "[TRAIN] batch   850/1000 |  85.0% | loss 5.5763 | 4.02 batches/s | ETA 0.7 min\n",
      "[TRAIN] batch   900/1000 |  90.0% | loss 5.5730 | 4.07 batches/s | ETA 0.5 min\n",
      "[TRAIN] batch   950/1000 |  95.0% | loss 5.5699 | 3.69 batches/s | ETA 0.2 min\n",
      "[TRAIN] batch  1000/1000 | 100.0% | loss 5.5667 | 3.66 batches/s | ETA 0.0 min\n",
      "→ first batch OK\n",
      "[VAL] batch    50/300 |  16.7% | loss 5.3450 | 12.76 batches/s | ETA 0.3 min\n",
      "[VAL] batch   100/300 |  33.3% | loss 5.3352 | 12.88 batches/s | ETA 0.3 min\n",
      "[VAL] batch   150/300 |  50.0% | loss 5.3425 | 12.57 batches/s | ETA 0.2 min\n",
      "[VAL] batch   200/300 |  66.7% | loss 5.3384 | 13.13 batches/s | ETA 0.1 min\n",
      "[VAL] batch   250/300 |  83.3% | loss 5.3426 | 13.32 batches/s | ETA 0.1 min\n",
      "[VAL] batch   300/300 | 100.0% | loss 5.3481 | 13.12 batches/s | ETA 0.0 min\n",
      "\n",
      " Résumé Epoch 3 | train_loss=5.5667 | val_loss=5.3481\n",
      " Nouveau meilleur modèle sauvegardé: ./best_transformer_summarizer.pt\n",
      "\n",
      "==========================================================================================\n",
      "Epoch 4/12 (limité à 1000 batches train, 300 val)\n",
      "→ first batch OK\n",
      "[TRAIN] batch    50/1000 |   5.0% | loss 5.5133 | 4.14 batches/s | ETA 3.8 min\n",
      "[TRAIN] batch   100/1000 |  10.0% | loss 5.5028 | 3.71 batches/s | ETA 3.8 min\n",
      "[TRAIN] batch   150/1000 |  15.0% | loss 5.5007 | 4.13 batches/s | ETA 3.6 min\n",
      "[TRAIN] batch   200/1000 |  20.0% | loss 5.4957 | 3.80 batches/s | ETA 3.4 min\n",
      "[TRAIN] batch   250/1000 |  25.0% | loss 5.4931 | 4.10 batches/s | ETA 3.2 min\n",
      "[TRAIN] batch   300/1000 |  30.0% | loss 5.4909 | 4.08 batches/s | ETA 2.9 min\n",
      "[TRAIN] batch   350/1000 |  35.0% | loss 5.4837 | 4.16 batches/s | ETA 2.7 min\n",
      "[TRAIN] batch   400/1000 |  40.0% | loss 5.4836 | 4.08 batches/s | ETA 2.5 min\n",
      "[TRAIN] batch   450/1000 |  45.0% | loss 5.4800 | 3.59 batches/s | ETA 2.3 min\n",
      "[TRAIN] batch   500/1000 |  50.0% | loss 5.4825 | 3.99 batches/s | ETA 2.1 min\n",
      "[TRAIN] batch   550/1000 |  55.0% | loss 5.4774 | 4.16 batches/s | ETA 1.9 min\n",
      "[TRAIN] batch   600/1000 |  60.0% | loss 5.4708 | 4.13 batches/s | ETA 1.7 min\n",
      "[TRAIN] batch   650/1000 |  65.0% | loss 5.4675 | 3.89 batches/s | ETA 1.5 min\n",
      "[TRAIN] batch   700/1000 |  70.0% | loss 5.4648 | 3.68 batches/s | ETA 1.3 min\n",
      "[TRAIN] batch   750/1000 |  75.0% | loss 5.4601 | 4.03 batches/s | ETA 1.0 min\n",
      "[TRAIN] batch   800/1000 |  80.0% | loss 5.4565 | 4.19 batches/s | ETA 0.8 min\n",
      "[TRAIN] batch   850/1000 |  85.0% | loss 5.4549 | 4.27 batches/s | ETA 0.6 min\n",
      "[TRAIN] batch   900/1000 |  90.0% | loss 5.4520 | 4.26 batches/s | ETA 0.4 min\n",
      "[TRAIN] batch   950/1000 |  95.0% | loss 5.4482 | 4.10 batches/s | ETA 0.2 min\n",
      "[TRAIN] batch  1000/1000 | 100.0% | loss 5.4463 | 4.13 batches/s | ETA 0.0 min\n",
      "→ first batch OK\n",
      "[VAL] batch    50/300 |  16.7% | loss 5.2196 | 12.22 batches/s | ETA 0.3 min\n",
      "[VAL] batch   100/300 |  33.3% | loss 5.2117 | 8.47 batches/s | ETA 0.3 min\n",
      "[VAL] batch   150/300 |  50.0% | loss 5.2193 | 8.68 batches/s | ETA 0.3 min\n",
      "[VAL] batch   200/300 |  66.7% | loss 5.2163 | 9.81 batches/s | ETA 0.2 min\n",
      "[VAL] batch   250/300 |  83.3% | loss 5.2224 | 9.74 batches/s | ETA 0.1 min\n",
      "[VAL] batch   300/300 | 100.0% | loss 5.2277 | 9.97 batches/s | ETA 0.0 min\n",
      "\n",
      " Résumé Epoch 4 | train_loss=5.4463 | val_loss=5.2277\n",
      " Nouveau meilleur modèle sauvegardé: ./best_transformer_summarizer.pt\n",
      "\n",
      "==========================================================================================\n",
      "Epoch 5/12 (limité à 1000 batches train, 300 val)\n",
      "→ first batch OK\n",
      "[TRAIN] batch    50/1000 |   5.0% | loss 5.4061 | 3.16 batches/s | ETA 5.0 min\n",
      "[TRAIN] batch   100/1000 |  10.0% | loss 5.3829 | 3.34 batches/s | ETA 4.6 min\n",
      "[TRAIN] batch   150/1000 |  15.0% | loss 5.3749 | 3.35 batches/s | ETA 4.3 min\n",
      "[TRAIN] batch   200/1000 |  20.0% | loss 5.3674 | 3.32 batches/s | ETA 4.1 min\n",
      "[TRAIN] batch   250/1000 |  25.0% | loss 5.3671 | 3.31 batches/s | ETA 3.8 min\n",
      "[TRAIN] batch   300/1000 |  30.0% | loss 5.3672 | 3.31 batches/s | ETA 3.5 min\n",
      "[TRAIN] batch   350/1000 |  35.0% | loss 5.3690 | 3.26 batches/s | ETA 3.3 min\n",
      "[TRAIN] batch   400/1000 |  40.0% | loss 5.3662 | 3.30 batches/s | ETA 3.0 min\n",
      "[TRAIN] batch   450/1000 |  45.0% | loss 5.3628 | 3.30 batches/s | ETA 2.8 min\n",
      "[TRAIN] batch   500/1000 |  50.0% | loss 5.3617 | 3.28 batches/s | ETA 2.5 min\n",
      "[TRAIN] batch   550/1000 |  55.0% | loss 5.3584 | 3.75 batches/s | ETA 2.3 min\n",
      "[TRAIN] batch   600/1000 |  60.0% | loss 5.3564 | 4.03 batches/s | ETA 2.0 min\n",
      "[TRAIN] batch   650/1000 |  65.0% | loss 5.3552 | 4.04 batches/s | ETA 1.7 min\n",
      "[TRAIN] batch   700/1000 |  70.0% | loss 5.3521 | 4.30 batches/s | ETA 1.4 min\n",
      "[TRAIN] batch   750/1000 |  75.0% | loss 5.3489 | 3.65 batches/s | ETA 1.2 min\n",
      "[TRAIN] batch   800/1000 |  80.0% | loss 5.3453 | 3.39 batches/s | ETA 1.0 min\n",
      "[TRAIN] batch   850/1000 |  85.0% | loss 5.3447 | 3.71 batches/s | ETA 0.7 min\n",
      "[TRAIN] batch   900/1000 |  90.0% | loss 5.3408 | 3.72 batches/s | ETA 0.5 min\n",
      "[TRAIN] batch   950/1000 |  95.0% | loss 5.3404 | 3.74 batches/s | ETA 0.2 min\n",
      "[TRAIN] batch  1000/1000 | 100.0% | loss 5.3391 | 4.27 batches/s | ETA 0.0 min\n",
      "→ first batch OK\n",
      "[VAL] batch    50/300 |  16.7% | loss 5.1311 | 13.37 batches/s | ETA 0.3 min\n",
      "[VAL] batch   100/300 |  33.3% | loss 5.1218 | 13.49 batches/s | ETA 0.2 min\n",
      "[VAL] batch   150/300 |  50.0% | loss 5.1289 | 11.61 batches/s | ETA 0.2 min\n",
      "[VAL] batch   200/300 |  66.7% | loss 5.1250 | 12.00 batches/s | ETA 0.1 min\n",
      "[VAL] batch   250/300 |  83.3% | loss 5.1317 | 13.08 batches/s | ETA 0.1 min\n",
      "[VAL] batch   300/300 | 100.0% | loss 5.1362 | 12.65 batches/s | ETA 0.0 min\n",
      "\n",
      " Résumé Epoch 5 | train_loss=5.3391 | val_loss=5.1362\n",
      " Nouveau meilleur modèle sauvegardé: ./best_transformer_summarizer.pt\n",
      "\n",
      "==========================================================================================\n",
      "Epoch 6/12 (limité à 1000 batches train, 300 val)\n",
      "→ first batch OK\n",
      "[TRAIN] batch    50/1000 |   5.0% | loss 5.2850 | 4.37 batches/s | ETA 3.6 min\n",
      "[TRAIN] batch   100/1000 |  10.0% | loss 5.2859 | 4.52 batches/s | ETA 3.4 min\n",
      "[TRAIN] batch   150/1000 |  15.0% | loss 5.2881 | 4.51 batches/s | ETA 3.2 min\n",
      "[TRAIN] batch   200/1000 |  20.0% | loss 5.2850 | 4.54 batches/s | ETA 3.0 min\n",
      "[TRAIN] batch   250/1000 |  25.0% | loss 5.2805 | 3.91 batches/s | ETA 2.9 min\n",
      "[TRAIN] batch   300/1000 |  30.0% | loss 5.2796 | 4.35 batches/s | ETA 2.7 min\n",
      "[TRAIN] batch   350/1000 |  35.0% | loss 5.2805 | 4.03 batches/s | ETA 2.5 min\n",
      "[TRAIN] batch   400/1000 |  40.0% | loss 5.2769 | 4.12 batches/s | ETA 2.3 min\n",
      "[TRAIN] batch   450/1000 |  45.0% | loss 5.2765 | 4.07 batches/s | ETA 2.2 min\n",
      "[TRAIN] batch   500/1000 |  50.0% | loss 5.2752 | 4.13 batches/s | ETA 2.0 min\n",
      "[TRAIN] batch   550/1000 |  55.0% | loss 5.2774 | 4.21 batches/s | ETA 1.8 min\n",
      "[TRAIN] batch   600/1000 |  60.0% | loss 5.2741 | 3.98 batches/s | ETA 1.6 min\n",
      "[TRAIN] batch   650/1000 |  65.0% | loss 5.2683 | 3.76 batches/s | ETA 1.4 min\n",
      "[TRAIN] batch   700/1000 |  70.0% | loss 5.2655 | 3.51 batches/s | ETA 1.2 min\n",
      "[TRAIN] batch   750/1000 |  75.0% | loss 5.2672 | 3.71 batches/s | ETA 1.0 min\n",
      "[TRAIN] batch   800/1000 |  80.0% | loss 5.2646 | 3.34 batches/s | ETA 0.8 min\n",
      "[TRAIN] batch   850/1000 |  85.0% | loss 5.2640 | 3.09 batches/s | ETA 0.6 min\n",
      "[TRAIN] batch   900/1000 |  90.0% | loss 5.2618 | 3.96 batches/s | ETA 0.4 min\n",
      "[TRAIN] batch   950/1000 |  95.0% | loss 5.2604 | 3.92 batches/s | ETA 0.2 min\n",
      "[TRAIN] batch  1000/1000 | 100.0% | loss 5.2588 | 4.13 batches/s | ETA 0.0 min\n",
      "→ first batch OK\n",
      "[VAL] batch    50/300 |  16.7% | loss 5.0580 | 12.14 batches/s | ETA 0.3 min\n",
      "[VAL] batch   100/300 |  33.3% | loss 5.0508 | 11.17 batches/s | ETA 0.3 min\n",
      "[VAL] batch   150/300 |  50.0% | loss 5.0590 | 10.88 batches/s | ETA 0.2 min\n",
      "[VAL] batch   200/300 |  66.7% | loss 5.0550 | 11.96 batches/s | ETA 0.1 min\n",
      "[VAL] batch   250/300 |  83.3% | loss 5.0608 | 12.84 batches/s | ETA 0.1 min\n",
      "[VAL] batch   300/300 | 100.0% | loss 5.0658 | 13.06 batches/s | ETA 0.0 min\n",
      "\n",
      " Résumé Epoch 6 | train_loss=5.2588 | val_loss=5.0658\n",
      " Nouveau meilleur modèle sauvegardé: ./best_transformer_summarizer.pt\n",
      "\n",
      "==========================================================================================\n",
      "Epoch 7/12 (limité à 1000 batches train, 300 val)\n",
      "→ first batch OK\n",
      "[TRAIN] batch    50/1000 |   5.0% | loss 5.2208 | 4.14 batches/s | ETA 3.8 min\n",
      "[TRAIN] batch   100/1000 |  10.0% | loss 5.1973 | 4.07 batches/s | ETA 3.7 min\n",
      "[TRAIN] batch   150/1000 |  15.0% | loss 5.2017 | 3.76 batches/s | ETA 3.6 min\n",
      "[TRAIN] batch   200/1000 |  20.0% | loss 5.2030 | 4.26 batches/s | ETA 3.3 min\n",
      "[TRAIN] batch   250/1000 |  25.0% | loss 5.2042 | 3.79 batches/s | ETA 3.1 min\n",
      "[TRAIN] batch   300/1000 |  30.0% | loss 5.2019 | 3.66 batches/s | ETA 3.0 min\n",
      "[TRAIN] batch   350/1000 |  35.0% | loss 5.2019 | 4.00 batches/s | ETA 2.7 min\n",
      "[TRAIN] batch   400/1000 |  40.0% | loss 5.2023 | 3.97 batches/s | ETA 2.5 min\n",
      "[TRAIN] batch   450/1000 |  45.0% | loss 5.2054 | 3.98 batches/s | ETA 2.3 min\n",
      "[TRAIN] batch   500/1000 |  50.0% | loss 5.2035 | 3.99 batches/s | ETA 2.1 min\n",
      "[TRAIN] batch   550/1000 |  55.0% | loss 5.2016 | 3.33 batches/s | ETA 1.9 min\n",
      "[TRAIN] batch   600/1000 |  60.0% | loss 5.2000 | 3.77 batches/s | ETA 1.7 min\n",
      "[TRAIN] batch   650/1000 |  65.0% | loss 5.1979 | 3.50 batches/s | ETA 1.5 min\n",
      "[TRAIN] batch   700/1000 |  70.0% | loss 5.1971 | 3.80 batches/s | ETA 1.3 min\n",
      "[TRAIN] batch   750/1000 |  75.0% | loss 5.1953 | 3.85 batches/s | ETA 1.1 min\n",
      "[TRAIN] batch   800/1000 |  80.0% | loss 5.1909 | 3.51 batches/s | ETA 0.9 min\n",
      "[TRAIN] batch   850/1000 |  85.0% | loss 5.1900 | 3.87 batches/s | ETA 0.7 min\n",
      "[TRAIN] batch   900/1000 |  90.0% | loss 5.1877 | 4.26 batches/s | ETA 0.4 min\n",
      "[TRAIN] batch   950/1000 |  95.0% | loss 5.1867 | 4.19 batches/s | ETA 0.2 min\n",
      "[TRAIN] batch  1000/1000 | 100.0% | loss 5.1851 | 3.01 batches/s | ETA 0.0 min\n",
      "→ first batch OK\n",
      "[VAL] batch    50/300 |  16.7% | loss 4.9919 | 11.35 batches/s | ETA 0.4 min\n",
      "[VAL] batch   100/300 |  33.3% | loss 4.9789 | 11.91 batches/s | ETA 0.3 min\n",
      "[VAL] batch   150/300 |  50.0% | loss 4.9870 | 11.79 batches/s | ETA 0.2 min\n",
      "[VAL] batch   200/300 |  66.7% | loss 4.9836 | 11.71 batches/s | ETA 0.1 min\n",
      "[VAL] batch   250/300 |  83.3% | loss 4.9896 | 11.71 batches/s | ETA 0.1 min\n",
      "[VAL] batch   300/300 | 100.0% | loss 4.9937 | 11.62 batches/s | ETA 0.0 min\n",
      "\n",
      " Résumé Epoch 7 | train_loss=5.1851 | val_loss=4.9937\n",
      " Nouveau meilleur modèle sauvegardé: ./best_transformer_summarizer.pt\n",
      "\n",
      "==========================================================================================\n",
      "Epoch 8/12 (limité à 1000 batches train, 300 val)\n",
      "→ first batch OK\n",
      "[TRAIN] batch    50/1000 |   5.0% | loss 5.1695 | 3.59 batches/s | ETA 4.4 min\n",
      "[TRAIN] batch   100/1000 |  10.0% | loss 5.1448 | 3.82 batches/s | ETA 4.1 min\n",
      "[TRAIN] batch   150/1000 |  15.0% | loss 5.1381 | 4.10 batches/s | ETA 3.7 min\n",
      "[TRAIN] batch   200/1000 |  20.0% | loss 5.1363 | 4.00 batches/s | ETA 3.4 min\n",
      "[TRAIN] batch   250/1000 |  25.0% | loss 5.1428 | 4.19 batches/s | ETA 3.2 min\n",
      "[TRAIN] batch   300/1000 |  30.0% | loss 5.1375 | 3.91 batches/s | ETA 3.0 min\n",
      "[TRAIN] batch   350/1000 |  35.0% | loss 5.1356 | 3.61 batches/s | ETA 2.8 min\n",
      "[TRAIN] batch   400/1000 |  40.0% | loss 5.1394 | 3.91 batches/s | ETA 2.6 min\n",
      "[TRAIN] batch   450/1000 |  45.0% | loss 5.1392 | 3.70 batches/s | ETA 2.4 min\n",
      "[TRAIN] batch   500/1000 |  50.0% | loss 5.1392 | 3.91 batches/s | ETA 2.2 min\n",
      "[TRAIN] batch   550/1000 |  55.0% | loss 5.1398 | 4.20 batches/s | ETA 1.9 min\n",
      "[TRAIN] batch   600/1000 |  60.0% | loss 5.1407 | 4.13 batches/s | ETA 1.7 min\n",
      "[TRAIN] batch   650/1000 |  65.0% | loss 5.1396 | 3.81 batches/s | ETA 1.5 min\n",
      "[TRAIN] batch   700/1000 |  70.0% | loss 5.1383 | 4.29 batches/s | ETA 1.3 min\n",
      "[TRAIN] batch   750/1000 |  75.0% | loss 5.1355 | 4.18 batches/s | ETA 1.1 min\n",
      "[TRAIN] batch   800/1000 |  80.0% | loss 5.1331 | 3.51 batches/s | ETA 0.9 min\n",
      "[TRAIN] batch   850/1000 |  85.0% | loss 5.1329 | 3.95 batches/s | ETA 0.6 min\n",
      "[TRAIN] batch   900/1000 |  90.0% | loss 5.1321 | 3.86 batches/s | ETA 0.4 min\n",
      "[TRAIN] batch   950/1000 |  95.0% | loss 5.1325 | 3.89 batches/s | ETA 0.2 min\n",
      "[TRAIN] batch  1000/1000 | 100.0% | loss 5.1289 | 3.90 batches/s | ETA 0.0 min\n",
      "→ first batch OK\n",
      "[VAL] batch    50/300 |  16.7% | loss 4.9423 | 12.15 batches/s | ETA 0.3 min\n",
      "[VAL] batch   100/300 |  33.3% | loss 4.9324 | 12.21 batches/s | ETA 0.3 min\n",
      "[VAL] batch   150/300 |  50.0% | loss 4.9419 | 12.53 batches/s | ETA 0.2 min\n",
      "[VAL] batch   200/300 |  66.7% | loss 4.9373 | 13.31 batches/s | ETA 0.1 min\n",
      "[VAL] batch   250/300 |  83.3% | loss 4.9451 | 13.47 batches/s | ETA 0.1 min\n",
      "[VAL] batch   300/300 | 100.0% | loss 4.9490 | 10.84 batches/s | ETA 0.0 min\n",
      "\n",
      " Résumé Epoch 8 | train_loss=5.1289 | val_loss=4.9490\n",
      " Nouveau meilleur modèle sauvegardé: ./best_transformer_summarizer.pt\n",
      "\n",
      "==========================================================================================\n",
      "Epoch 9/12 (limité à 1000 batches train, 300 val)\n",
      "→ first batch OK\n",
      "[TRAIN] batch    50/1000 |   5.0% | loss 5.0992 | 3.70 batches/s | ETA 4.3 min\n",
      "[TRAIN] batch   100/1000 |  10.0% | loss 5.0921 | 4.09 batches/s | ETA 3.9 min\n",
      "[TRAIN] batch   150/1000 |  15.0% | loss 5.0822 | 4.19 batches/s | ETA 3.6 min\n",
      "[TRAIN] batch   200/1000 |  20.0% | loss 5.0854 | 4.46 batches/s | ETA 3.3 min\n",
      "[TRAIN] batch   250/1000 |  25.0% | loss 5.0903 | 3.49 batches/s | ETA 3.2 min\n",
      "[TRAIN] batch   300/1000 |  30.0% | loss 5.0928 | 3.50 batches/s | ETA 3.0 min\n",
      "[TRAIN] batch   350/1000 |  35.0% | loss 5.0869 | 3.87 batches/s | ETA 2.8 min\n",
      "[TRAIN] batch   400/1000 |  40.0% | loss 5.0882 | 4.22 batches/s | ETA 2.6 min\n",
      "[TRAIN] batch   450/1000 |  45.0% | loss 5.0899 | 3.93 batches/s | ETA 2.3 min\n",
      "[TRAIN] batch   500/1000 |  50.0% | loss 5.0913 | 4.06 batches/s | ETA 2.1 min\n",
      "[TRAIN] batch   550/1000 |  55.0% | loss 5.0902 | 3.72 batches/s | ETA 1.9 min\n",
      "[TRAIN] batch   600/1000 |  60.0% | loss 5.0888 | 3.12 batches/s | ETA 1.7 min\n",
      "[TRAIN] batch   650/1000 |  65.0% | loss 5.0859 | 3.67 batches/s | ETA 1.5 min\n",
      "[TRAIN] batch   700/1000 |  70.0% | loss 5.0802 | 3.70 batches/s | ETA 1.3 min\n",
      "[TRAIN] batch   750/1000 |  75.0% | loss 5.0784 | 3.60 batches/s | ETA 1.1 min\n",
      "[TRAIN] batch   800/1000 |  80.0% | loss 5.0782 | 4.20 batches/s | ETA 0.9 min\n",
      "[TRAIN] batch   850/1000 |  85.0% | loss 5.0765 | 3.33 batches/s | ETA 0.7 min\n",
      "[TRAIN] batch   900/1000 |  90.0% | loss 5.0756 | 3.71 batches/s | ETA 0.4 min\n",
      "[TRAIN] batch   950/1000 |  95.0% | loss 5.0754 | 4.23 batches/s | ETA 0.2 min\n",
      "[TRAIN] batch  1000/1000 | 100.0% | loss 5.0747 | 4.27 batches/s | ETA 0.0 min\n",
      "→ first batch OK\n",
      "[VAL] batch    50/300 |  16.7% | loss 4.9001 | 12.85 batches/s | ETA 0.3 min\n",
      "[VAL] batch   100/300 |  33.3% | loss 4.8896 | 13.10 batches/s | ETA 0.3 min\n",
      "[VAL] batch   150/300 |  50.0% | loss 4.8981 | 13.40 batches/s | ETA 0.2 min\n",
      "[VAL] batch   200/300 |  66.7% | loss 4.8941 | 13.32 batches/s | ETA 0.1 min\n",
      "[VAL] batch   250/300 |  83.3% | loss 4.9017 | 13.21 batches/s | ETA 0.1 min\n",
      "[VAL] batch   300/300 | 100.0% | loss 4.9056 | 14.11 batches/s | ETA 0.0 min\n",
      "\n",
      " Résumé Epoch 9 | train_loss=5.0747 | val_loss=4.9056\n",
      " Nouveau meilleur modèle sauvegardé: ./best_transformer_summarizer.pt\n",
      "\n",
      "==========================================================================================\n",
      "Epoch 10/12 (limité à 1000 batches train, 300 val)\n",
      "→ first batch OK\n",
      "[TRAIN] batch    50/1000 |   5.0% | loss 5.0609 | 4.30 batches/s | ETA 3.7 min\n",
      "[TRAIN] batch   100/1000 |  10.0% | loss 5.0411 | 4.11 batches/s | ETA 3.6 min\n",
      "[TRAIN] batch   150/1000 |  15.0% | loss 5.0458 | 4.10 batches/s | ETA 3.4 min\n",
      "[TRAIN] batch   200/1000 |  20.0% | loss 5.0501 | 4.09 batches/s | ETA 3.2 min\n",
      "[TRAIN] batch   250/1000 |  25.0% | loss 5.0438 | 4.09 batches/s | ETA 3.0 min\n",
      "[TRAIN] batch   300/1000 |  30.0% | loss 5.0433 | 4.05 batches/s | ETA 2.8 min\n",
      "[TRAIN] batch   350/1000 |  35.0% | loss 5.0454 | 4.09 batches/s | ETA 2.6 min\n",
      "[TRAIN] batch   400/1000 |  40.0% | loss 5.0416 | 4.24 batches/s | ETA 2.4 min\n",
      "[TRAIN] batch   450/1000 |  45.0% | loss 5.0383 | 4.09 batches/s | ETA 2.2 min\n",
      "[TRAIN] batch   500/1000 |  50.0% | loss 5.0382 | 4.00 batches/s | ETA 2.0 min\n",
      "[TRAIN] batch   550/1000 |  55.0% | loss 5.0376 | 3.65 batches/s | ETA 1.8 min\n",
      "[TRAIN] batch   600/1000 |  60.0% | loss 5.0377 | 4.10 batches/s | ETA 1.6 min\n",
      "[TRAIN] batch   650/1000 |  65.0% | loss 5.0349 | 4.10 batches/s | ETA 1.4 min\n",
      "[TRAIN] batch   700/1000 |  70.0% | loss 5.0336 | 3.96 batches/s | ETA 1.2 min\n",
      "[TRAIN] batch   750/1000 |  75.0% | loss 5.0316 | 3.62 batches/s | ETA 1.0 min\n",
      "[TRAIN] batch   800/1000 |  80.0% | loss 5.0328 | 3.21 batches/s | ETA 0.8 min\n",
      "[TRAIN] batch   850/1000 |  85.0% | loss 5.0321 | 4.16 batches/s | ETA 0.6 min\n",
      "[TRAIN] batch   900/1000 |  90.0% | loss 5.0330 | 4.07 batches/s | ETA 0.4 min\n",
      "[TRAIN] batch   950/1000 |  95.0% | loss 5.0326 | 3.90 batches/s | ETA 0.2 min\n",
      "[TRAIN] batch  1000/1000 | 100.0% | loss 5.0339 | 2.85 batches/s | ETA 0.0 min\n",
      "→ first batch OK\n",
      "[VAL] batch    50/300 |  16.7% | loss 4.8553 | 9.68 batches/s | ETA 0.4 min\n",
      "[VAL] batch   100/300 |  33.3% | loss 4.8482 | 11.94 batches/s | ETA 0.3 min\n",
      "[VAL] batch   150/300 |  50.0% | loss 4.8580 | 13.56 batches/s | ETA 0.2 min\n",
      "[VAL] batch   200/300 |  66.7% | loss 4.8542 | 13.49 batches/s | ETA 0.1 min\n",
      "[VAL] batch   250/300 |  83.3% | loss 4.8614 | 13.84 batches/s | ETA 0.1 min\n",
      "[VAL] batch   300/300 | 100.0% | loss 4.8649 | 12.88 batches/s | ETA 0.0 min\n",
      "\n",
      " Résumé Epoch 10 | train_loss=5.0339 | val_loss=4.8649\n",
      " Nouveau meilleur modèle sauvegardé: ./best_transformer_summarizer.pt\n",
      "\n",
      "==========================================================================================\n",
      "Epoch 11/12 (limité à 1000 batches train, 300 val)\n",
      "→ first batch OK\n",
      "[TRAIN] batch    50/1000 |   5.0% | loss 5.0175 | 4.03 batches/s | ETA 3.9 min\n",
      "[TRAIN] batch   100/1000 |  10.0% | loss 5.0105 | 4.26 batches/s | ETA 3.6 min\n",
      "[TRAIN] batch   150/1000 |  15.0% | loss 5.0120 | 4.22 batches/s | ETA 3.4 min\n",
      "[TRAIN] batch   200/1000 |  20.0% | loss 5.0033 | 4.40 batches/s | ETA 3.2 min\n",
      "[TRAIN] batch   250/1000 |  25.0% | loss 5.0042 | 4.41 batches/s | ETA 2.9 min\n",
      "[TRAIN] batch   300/1000 |  30.0% | loss 4.9999 | 4.12 batches/s | ETA 2.8 min\n",
      "[TRAIN] batch   350/1000 |  35.0% | loss 5.0008 | 3.80 batches/s | ETA 2.6 min\n",
      "[TRAIN] batch   400/1000 |  40.0% | loss 4.9914 | 4.12 batches/s | ETA 2.4 min\n",
      "[TRAIN] batch   450/1000 |  45.0% | loss 4.9955 | 4.29 batches/s | ETA 2.2 min\n",
      "[TRAIN] batch   500/1000 |  50.0% | loss 4.9943 | 4.15 batches/s | ETA 2.0 min\n",
      "[TRAIN] batch   550/1000 |  55.0% | loss 4.9963 | 4.25 batches/s | ETA 1.8 min\n",
      "[TRAIN] batch   600/1000 |  60.0% | loss 4.9980 | 4.55 batches/s | ETA 1.6 min\n",
      "[TRAIN] batch   650/1000 |  65.0% | loss 4.9959 | 4.02 batches/s | ETA 1.4 min\n",
      "[TRAIN] batch   700/1000 |  70.0% | loss 4.9960 | 4.38 batches/s | ETA 1.2 min\n",
      "[TRAIN] batch   750/1000 |  75.0% | loss 4.9970 | 4.12 batches/s | ETA 1.0 min\n",
      "[TRAIN] batch   800/1000 |  80.0% | loss 4.9990 | 4.24 batches/s | ETA 0.8 min\n",
      "[TRAIN] batch   850/1000 |  85.0% | loss 4.9981 | 4.01 batches/s | ETA 0.6 min\n",
      "[TRAIN] batch   900/1000 |  90.0% | loss 4.9990 | 3.45 batches/s | ETA 0.4 min\n",
      "[TRAIN] batch   950/1000 |  95.0% | loss 4.9977 | 4.37 batches/s | ETA 0.2 min\n",
      "[TRAIN] batch  1000/1000 | 100.0% | loss 4.9984 | 4.55 batches/s | ETA 0.0 min\n",
      "→ first batch OK\n",
      "[VAL] batch    50/300 |  16.7% | loss 4.8290 | 13.73 batches/s | ETA 0.3 min\n",
      "[VAL] batch   100/300 |  33.3% | loss 4.8202 | 13.67 batches/s | ETA 0.2 min\n",
      "[VAL] batch   150/300 |  50.0% | loss 4.8277 | 12.70 batches/s | ETA 0.2 min\n",
      "[VAL] batch   200/300 |  66.7% | loss 4.8223 | 13.15 batches/s | ETA 0.1 min\n",
      "[VAL] batch   250/300 |  83.3% | loss 4.8279 | 11.79 batches/s | ETA 0.1 min\n",
      "[VAL] batch   300/300 | 100.0% | loss 4.8308 | 11.79 batches/s | ETA 0.0 min\n",
      "\n",
      " Résumé Epoch 11 | train_loss=4.9984 | val_loss=4.8308\n",
      " Nouveau meilleur modèle sauvegardé: ./best_transformer_summarizer.pt\n",
      "\n",
      "==========================================================================================\n",
      "Epoch 12/12 (limité à 1000 batches train, 300 val)\n",
      "→ first batch OK\n",
      "[TRAIN] batch    50/1000 |   5.0% | loss 4.9690 | 4.06 batches/s | ETA 3.9 min\n",
      "[TRAIN] batch   100/1000 |  10.0% | loss 4.9682 | 3.04 batches/s | ETA 4.3 min\n",
      "[TRAIN] batch   150/1000 |  15.0% | loss 4.9821 | 3.31 batches/s | ETA 4.1 min\n",
      "[TRAIN] batch   200/1000 |  20.0% | loss 4.9821 | 3.46 batches/s | ETA 3.9 min\n",
      "[TRAIN] batch   250/1000 |  25.0% | loss 4.9804 | 4.26 batches/s | ETA 3.5 min\n",
      "[TRAIN] batch   300/1000 |  30.0% | loss 4.9749 | 4.44 batches/s | ETA 3.2 min\n",
      "[TRAIN] batch   350/1000 |  35.0% | loss 4.9740 | 4.27 batches/s | ETA 2.9 min\n",
      "[TRAIN] batch   400/1000 |  40.0% | loss 4.9709 | 4.24 batches/s | ETA 2.6 min\n",
      "[TRAIN] batch   450/1000 |  45.0% | loss 4.9688 | 3.06 batches/s | ETA 2.5 min\n",
      "[TRAIN] batch   500/1000 |  50.0% | loss 4.9686 | 3.85 batches/s | ETA 2.2 min\n",
      "[TRAIN] batch   550/1000 |  55.0% | loss 4.9680 | 3.78 batches/s | ETA 2.0 min\n",
      "[TRAIN] batch   600/1000 |  60.0% | loss 4.9658 | 3.68 batches/s | ETA 1.8 min\n",
      "[TRAIN] batch   650/1000 |  65.0% | loss 4.9626 | 3.84 batches/s | ETA 1.6 min\n",
      "[TRAIN] batch   700/1000 |  70.0% | loss 4.9622 | 4.23 batches/s | ETA 1.3 min\n",
      "[TRAIN] batch   750/1000 |  75.0% | loss 4.9631 | 4.31 batches/s | ETA 1.1 min\n",
      "[TRAIN] batch   800/1000 |  80.0% | loss 4.9645 | 4.41 batches/s | ETA 0.9 min\n",
      "[TRAIN] batch   850/1000 |  85.0% | loss 4.9640 | 4.33 batches/s | ETA 0.6 min\n",
      "[TRAIN] batch   900/1000 |  90.0% | loss 4.9623 | 4.33 batches/s | ETA 0.4 min\n",
      "[TRAIN] batch   950/1000 |  95.0% | loss 4.9610 | 4.11 batches/s | ETA 0.2 min\n",
      "[TRAIN] batch  1000/1000 | 100.0% | loss 4.9608 | 4.15 batches/s | ETA 0.0 min\n",
      "→ first batch OK\n",
      "[VAL] batch    50/300 |  16.7% | loss 4.7911 | 13.05 batches/s | ETA 0.3 min\n",
      "[VAL] batch   100/300 |  33.3% | loss 4.7837 | 13.29 batches/s | ETA 0.3 min\n",
      "[VAL] batch   150/300 |  50.0% | loss 4.7932 | 10.86 batches/s | ETA 0.2 min\n",
      "[VAL] batch   200/300 |  66.7% | loss 4.7883 | 10.22 batches/s | ETA 0.1 min\n",
      "[VAL] batch   250/300 |  83.3% | loss 4.7955 | 9.32 batches/s | ETA 0.1 min\n",
      "[VAL] batch   300/300 | 100.0% | loss 4.7985 | 9.51 batches/s | ETA 0.0 min\n",
      "\n",
      " Résumé Epoch 12 | train_loss=4.9608 | val_loss=4.7985\n",
      " Nouveau meilleur modèle sauvegardé: ./best_transformer_summarizer.pt\n",
      "\n",
      " Entraînement terminé\n"
     ]
    }
   ],
   "source": [
    "print(\" Début entraînement (batch limitées)\")\n",
    "print(\"Device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} (limité à {MAX_TRAIN_BATCHES} batches train, {MAX_VAL_BATCHES} val)\")\n",
    "\n",
    "    train_loss = run_epoch_limited(\n",
    "        train_loader,\n",
    "        train=True,\n",
    "        log_every=50,\n",
    "        max_batches=MAX_TRAIN_BATCHES\n",
    "    )\n",
    "\n",
    "    val_loss = run_epoch_limited(\n",
    "        val_loader,\n",
    "        train=False,\n",
    "        log_every=50,\n",
    "        max_batches=MAX_VAL_BATCHES\n",
    "    )\n",
    "\n",
    "    print(f\"\\n Résumé Epoch {epoch} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save(model.state_dict(), BEST_PATH)\n",
    "        print(f\" Nouveau meilleur modèle sauvegardé: {BEST_PATH}\")\n",
    "\n",
    "print(\"\\n Entraînement terminé\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e0f58",
   "metadata": {},
   "source": [
    "## Génération (greedy) + décodage en texte\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12795a5e",
   "metadata": {},
   "source": [
    "On génère un résumé token par token (greedy = argmax).\n",
    "Pourquoi : simple, rapide, parfait pour vérifier que le modèle “parle” avant d'ajouter beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "498617c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aurel\\AppData\\Local\\Temp\\ipykernel_23204\\980151232.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(BEST_PATH, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate_greedy(src_ids, max_new_tokens=MAX_OUT_TOKENS):\n",
    "    model.eval()\n",
    "    src_ids = src_ids.to(device)\n",
    "\n",
    "    B = src_ids.size(0)\n",
    "    dec = torch.full((B, 1), BOS, dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(src_ids, dec)\n",
    "        next_tok = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        dec = torch.cat([dec, next_tok], dim=1)\n",
    "        if (next_tok.squeeze(-1) == EOS).all():\n",
    "            break\n",
    "    return dec\n",
    "\n",
    "def decode_tokens(ids):\n",
    "    out = []\n",
    "    for t in ids:\n",
    "        if t in (PAD, BOS): \n",
    "            continue\n",
    "        if t == EOS:\n",
    "            break\n",
    "        out.append(t)\n",
    "    return sp.decode_ids(out)\n",
    "\n",
    "# charger le meilleur\n",
    "model.load_state_dict(torch.load(BEST_PATH, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ae7190",
   "metadata": {},
   "source": [
    "## Évaluation ROUGE (rapide)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae6cd1",
   "metadata": {},
   "source": [
    "ROUGE mesure le recouvrement n-gram entre résumé généré et référence.\n",
    "On commence sur un nombre limité de batches pour aller vite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51a748a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating batch 51/50\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.16198808296126962,\n",
       " 'rouge2': 0.0246171041378079,\n",
       " 'rougeL': 0.12687686719087116}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=False)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_rouge(loader, n_batches=50):\n",
    "    model.eval()\n",
    "    acc = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "\n",
    "    for i, batch in enumerate(loader):\n",
    "        print(f\"Evaluating batch {i+1}/{n_batches}\", end=\"\\r\")\n",
    "        if i >= n_batches:\n",
    "            break\n",
    "        src = batch.src_ids.to(device)\n",
    "        gen = generate_greedy(src).cpu().numpy()\n",
    "        ref = batch.labels.numpy()\n",
    "\n",
    "        for b in range(gen.shape[0]):\n",
    "            pred_txt = decode_tokens(gen[b].tolist())\n",
    "            ref_txt  = decode_tokens(ref[b].tolist())\n",
    "            s = scorer.score(ref_txt, pred_txt)\n",
    "            for k in acc:\n",
    "                acc[k].append(s[k].fmeasure)\n",
    "\n",
    "    return {k: float(np.mean(v)) for k,v in acc.items()}\n",
    "\n",
    "eval_rouge(test_loader, n_batches=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dcc0ca",
   "metadata": {},
   "source": [
    "## Démo qualitative (3 exemples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c806e84",
   "metadata": {},
   "source": [
    "En soutenance, tu montres : titre, début du texte, résumé référence, résumé généré.\n",
    "Pourquoi : ça rend l'évaluation “humaine” et visible (au-delà des métriques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8653342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================\n",
      "TITLE: Présidentielle en Tunisie : les trois inconnues du second tour\n",
      "\n",
      "TEXT (début): A Tunis, le 18 septembre 2019. Zoubeir Souissi / REUTERS La Tunisie aborde dans l’incertitude le second tour de l’élection présidentielle, après le premier tour du dimanche 15 septembre, qui a placé en tête le juriste conservateur Kaïs Saïed (18,40 %), devant le magnat de la télévision Nabil Karoui (15,58 %). Bien des hypothèques demeurent sur la suite de ce processus électoral crucial pour la consolidation de la transition démocratique en Tunisie, berceau et seul rescapé de la vague des « printemps arabes ». Quand aura lieu le second tour ? Le tribunal administratif de Tunis a rejeté lundi 23 ...\n",
      "\n",
      "REF SUMMARY: Calendrier du scrutin, maintien en détention du candidat Nabil Karoui, influence des législatives… De nombreuses incertitudes marquent la campagne d’entre-deux-tours.\n",
      "\n",
      "PRED SUMMARY: Le président de la République a annoncé jeudi que le président de la République a été mis en examen pour « « incitation ».\n",
      "==============================================================================================================\n",
      "TITLE: « L’âge d’or de l’aéronautique vit ses dernières années de gloire »\n",
      "\n",
      "TEXT (début): Au Salon aéronautique de Paris, au Bourget, le 17 juin. Pascal Rossignol / REUTERS Pertes & profits. C’est grâce à Frank Whittle que tous les aéroports du monde – et en particulier celui du Bourget, où se tient le Salon aéronautique de Paris – résonnent de ce bruit assourdissant si singulier, celui des moteurs à réaction. Inventés avant-guerre et développés à la fin des années 1940 par l’obstiné ingénieur de la Royal Air Force, ils ont changé la face de l’aéronautique, ouvrant la voie au transport de masse sur de très longues distances. Cet âge d’or, qui trouve son apogée aujourd’hui avec ­l’e ...\n",
      "\n",
      "REF SUMMARY: Les contraintes environnementales condamnent le moteur à réaction, qui porte la croissance du secteur depuis l’après-guerre. Son remplaçant n’est pas encore trouvé. Un défi existentiel pour compagnies et constructeurs, explique Philippe Escande, éditorialiste économique au « Monde », dans sa chronique.\n",
      "\n",
      "PRED SUMMARY: Le fabricant de la compagnie aérienne a annoncé, jeudi, que le groupe de l’entreprise est en train de se faire face à l’avenir de la crise.\n",
      "==============================================================================================================\n",
      "TITLE: Pour LR, « un accord avec le RN représenterait la fin programmée de la droite conservatrice traditionnelle »\n",
      "\n",
      "TEXT (début): Le directeur de l’Observatoire des radicalités politiques, Jean-Yves Camus, explique pourquoi le Rassemblement national (RN) aura du mal à capitaliser sur la mauvaise passe que traverse le parti Les Républicains (LR). Le 26 mai, LR a subi un revers historique aux élections européennes en obtenant 8,5 % des voix. Puis Laurent Wauquiez a démissionné. Pensez-vous que le RN va pouvoir capitaliser sur l’effondrement de la droite ? Pour répondre à cette question, il faut partir du principe que les résultats d’une élection européenne ne sont pas prédictifs des prochaines municipales. Le résultat du R ...\n",
      "\n",
      "REF SUMMARY: Si LRM parvient à récupérer de nombreux élus et électeurs d’une droite en miettes, plusieurs obstacles empêchent le RN de faire de même, selon le spécialiste de l’extrême droite Jean-Yves Camus.\n",
      "\n",
      "PRED SUMMARY: Le candidat républicain a répondu à une nouvelle fois à l’élection présidentielle de 2016, à l’Assemblée nationale.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "@torch.no_grad()\n",
    "def show_examples(df, n=3):\n",
    "    model.eval()\n",
    "    idxs = random.sample(range(len(df)), n)\n",
    "\n",
    "    for i in idxs:\n",
    "        row = df.iloc[i]\n",
    "        src_str = make_src(row)\n",
    "\n",
    "        src_ids = [BOS] + sp.encode_as_ids(src_str)[:MAX_IN_TOKENS-2] + [EOS]\n",
    "        src_ids = torch.tensor([pad_to(src_ids, MAX_IN_TOKENS)], dtype=torch.long)\n",
    "\n",
    "        gen_ids = generate_greedy(src_ids)[0].tolist()\n",
    "        pred = decode_tokens(gen_ids)\n",
    "\n",
    "        print(\"=\"*110)\n",
    "        if \"title\" in df.columns:\n",
    "            print(\"TITLE:\", row[\"title\"])\n",
    "        print(\"\\nTEXT (début):\", str(row[\"text\"])[:600], \"...\")\n",
    "        print(\"\\nREF SUMMARY:\", str(row[\"summary\"]))\n",
    "        print(\"\\nPRED SUMMARY:\", pred)\n",
    "\n",
    "show_examples(test_df, n=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
